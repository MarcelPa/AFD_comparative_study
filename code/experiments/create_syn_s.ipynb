{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa94728-39f9-4c74-9f48-364dc62d8a5c",
   "metadata": {},
   "source": [
    "# SYN$^s$ Generation\n",
    "\n",
    "In this notebook, we generate the SYN$^s$ dataset.\n",
    "\n",
    "## Setup\n",
    "First, load all the files from the RWD dataset. Futhermore, set some configuration parameters if running on an HPC cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033df0c2-fd4c-450d-8c91-9e0789f685fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# for Jupyter notebooks: add the path of 'code' to allow importing module\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "data_path = \"../../data\"\n",
    "gt_path = \"../../data/ground_truth.csv\"\n",
    "results_path = \"../../results\"\n",
    "# batch_i is used to parallelize measuring datasets on the HPC cluster\n",
    "batch_i = int(os.getenv(\"PBS_ARRAYID\", 0))\n",
    "# workers is used to parallelize measuring candidate FDs using joblib\n",
    "workers = int(os.getenv(\"PBS_NUM_PPN\", 1))\n",
    "total_batches = 1 # total number of batches that will be run on the HPC\n",
    "# files per batch\n",
    "batch_size = 1\n",
    "# this will be doubled: each dataset will be created as an FD and an non-FD\n",
    "datasets_per_setting = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37dd735",
   "metadata": {},
   "source": [
    "## Define a method to generate SYN$^s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df346-7697-4f93-a34b-1ee2159d0c8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random \n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from synthetic_data import generator as gen\n",
    "from synthetic_data import utils as utils\n",
    "\n",
    "def create_predominant_RHS(n: int, skew_lookup: pd.DataFrame, max_skew: float = 10.0):\n",
    "    ''' Create a predominant RHS. Important settings: LHS skew lower than 1.0, RHS cardinality low, RHS skew higher than 1.0. '''\n",
    "    settings = {\n",
    "        'tuples': random.randint(100, 10000),\n",
    "        'lhs_cardinality': 0,\n",
    "        'rhs_cardinality': 0,\n",
    "        'lhs_dist_alpha': random.randint(58, 100) / 100,\n",
    "        'lhs_dist_beta': random.randint(10, 18) / 10,\n",
    "        'rhs_dist_alpha': 1.0,\n",
    "        'rhs_dist_beta': 1.0,\n",
    "        'noise': random.uniform(0.005, 0.02),\n",
    "    }\n",
    "    settings['lhs_cardinality'] = random.randint(int(settings['tuples'] * .20), int(settings['tuples'] * .75))\n",
    "    while sd.beta_skewness(settings[f'lhs_dist_alpha'], settings[f'lhs_dist_beta']) > 0.5:\n",
    "        settings[f'lhs_dist_alpha'] = random.randint(58, 100) / 100\n",
    "        settings[f'lhs_dist_beta'] = random.randint(10, 18) / 10\n",
    "    df_set = []\n",
    "    for rhs_pred in range(0, n):\n",
    "        lower_skew = (max_skew * rhs_pred) / n\n",
    "        upper_skew = (max_skew * rhs_pred + max_skew) / n\n",
    "        alpha, beta = random.choice(skew_lookup.query(f'skew > {lower_skew} and skew < {upper_skew}').index)\n",
    "        settings['rhs_dist_alpha'] = alpha\n",
    "        settings['rhs_dist_beta'] = beta\n",
    "        settings['rhs_cardinality'] = random.randint(5, int(settings['lhs_cardinality'] / 2))\n",
    "        for fd in (True, False):\n",
    "            settings['fd'] = fd\n",
    "            df_set.append((generate_SYN(**settings), copy.deepcopy(settings)))\n",
    "    return df_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015898a-6d7c-460f-a351-95004d94cf87",
   "metadata": {},
   "source": [
    "## Generate SYN$^s$\n",
    "\n",
    "Generate the data using the method defined above. Also, collect and infer the settings used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228efa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_data import inferrence\n",
    "\n",
    "with Parallel(n_jobs=workers) as parallel:\n",
    "    predominant_both = Parallel()(delayed(create_predominant_RHS)(datasets_per_setting) for _ in tqdm(range(batch_size)))\n",
    "\n",
    "predominant_dfs = []\n",
    "predominant_settings = []\n",
    "predominant_inferred = []\n",
    "for df_set in tqdm(predominant_both):\n",
    "    for df, setting in df_set:\n",
    "        predominant_dfs.append(df)\n",
    "        predominant_settings.append(setting)\n",
    "        predominant_inferred.append(\n",
    "            inferrence.infer_settings(df)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "for df_i, predominant_df in enumerate(predominant_dfs):\n",
    "    predominant_df.to_csv(os.path.join(data_path, 'syn_s', f'{batch_i}_{df_i}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a0d8d-6c46-41f4-a6ba-e07e55d95db3",
   "metadata": {},
   "source": [
    "## Calculate SYN AFD measures\n",
    "\n",
    "After generating the tables, calculate the AFD measure scores on the generated files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ce5b0-6777-4cec-97c6-44abc1718ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Any\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import tqdm\n",
    "\n",
    "from afd_measures import measures as afd_measures\n",
    "from afd_measures import utils as afd_utils\n",
    "\n",
    "measures = [\n",
    "    \"rho\",\n",
    "    \"g2\",\n",
    "    \"g3\",\n",
    "    \"g3_prime\",\n",
    "    \"fraction_of_information\",\n",
    "    \"reliable_fraction_of_information_prime\",\n",
    "    \"smoothed_fraction_of_information\",\n",
    "    \"g1\",\n",
    "    \"g1_prime\",\n",
    "    \"pdep\",\n",
    "    \"tau\",\n",
    "    \"mu_prime\",\n",
    "]\n",
    "\n",
    "\n",
    "def parallelize_measuring(df: pd.DataFrame, table: str, lhs: Any, rhs: Any):\n",
    "    result = {\n",
    "        \"table\": table,\n",
    "        \"lhs\": lhs,\n",
    "        \"rhs\": rhs,\n",
    "    }\n",
    "    _df = df.loc[:, [lhs, rhs]].dropna().copy()\n",
    "    if _df.empty:\n",
    "        result[\"empty\"] = True\n",
    "        return result\n",
    "    result[\"trivial_fd\"] = afd_utils.is_trivial_fd(_df, lhs, rhs)\n",
    "    result[\"exact_fd\"] = afd_utils.is_perfect_fd(_df, lhs, rhs)\n",
    "    for measure in measures:\n",
    "        result[measure] = (\n",
    "            getattr(afd_measures, measure)(_df, lhs, rhs)\n",
    "            if not result[\"trivial_fd\"]\n",
    "            else 1.0\n",
    "        )\n",
    "    return result\n",
    "\n",
    "\n",
    "to_calulate = [\n",
    "    (df, f'{batch_i}_{df_i}', 'lhs', 'rhs')\n",
    "    for df_i, df in enumerate(predominant_dfs)\n",
    "]\n",
    "predominant_results = Parallel(n_jobs=workers)(\n",
    "    delayed(parallelize_measuring)(*args) for args in tqdm.tqdm(to_calulate)\n",
    ")\n",
    "# filter out the empty candidate FDs\n",
    "predominant_results_df = pd.DataFrame(predominant_results)\n",
    "predominant_settings = pd.DataFrame(predominant_settings)\n",
    "predominant_inferred = pd.DataFrame(predominant_inferred)\n",
    "predominant_settings['table'] = predominant_settings.apply(lambda r: f'{batch_i}_{r.name}', axis='columns')\n",
    "predominant_inferred['table'] = predominant_inferred.apply(lambda r: f'{batch_i}_{r.name}', axis='columns')\n",
    "predominant_results_df = predominant_results_df.merge(predominant_settings, on='table', suffixes=('', '_set')).merge(predominant_inferred, on='table', suffixes=('', '_inferred')).copy()\n",
    "# store result to a CSV\n",
    "predominant_results_df.to_csv(os.path.join(results_path, f'syn_s_results_{batch_i}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
